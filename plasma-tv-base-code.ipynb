{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-blood",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import signal\n",
    "import math\n",
    "from scipy.io import readsav\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image, ImageFilter\n",
    "import matplotlib.animation as animation\n",
    "import diplib as dip\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-spray",
   "metadata": {},
   "source": [
    "### Functions for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_data(filename):\n",
    "    dat = readsav(filename)\n",
    "    print(dat.keys())\n",
    "    emission = dat['emission_structure']\n",
    "    return emission[0]\n",
    "\n",
    "def _find_index(arr,val):\n",
    "    return np.argmin(abs(arr-val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-theme",
   "metadata": {},
   "source": [
    "### Functions for enhancing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-oriental",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(data):\n",
    "    mn = data.mean()\n",
    "    std = data.std()\n",
    "    return((data-mn)/std)\n",
    "\n",
    "def rescale(data):\n",
    "    return (data-data.min())/(data.max()-data.min())\n",
    "\n",
    "def quantfilt(src,thr=0.9):\n",
    "    filt = np.quantile(src,thr,axis=0)\n",
    "    out = np.where(src<filt,0,src)\n",
    "    return out\n",
    "\n",
    "# gaussian filtering\n",
    "def gaussblr(src,filt=(31, 3)):\n",
    "    src = (rescale(src)*255).astype('uint8')\n",
    "    out = cv2.GaussianBlur(src,filt,0)\n",
    "    return rescale(out)\n",
    "\n",
    "# mean filtering\n",
    "def meansub(src):\n",
    "    mn = np.mean(src,axis=1)[:,np.newaxis]\n",
    "    out = np.absolute(src - mn)\n",
    "    return rescale(out)\n",
    "\n",
    "# morphological filtering\n",
    "def morph(src):\n",
    "    src = (rescale(src)*255).astype('uint8')\n",
    "    se1 = cv2.getStructuringElement(cv2.MORPH_RECT, (4,4))\n",
    "    se2 = cv2.getStructuringElement(cv2.MORPH_RECT, (3,1))\n",
    "    mask = cv2.morphologyEx(src, cv2.MORPH_CLOSE, se1)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, se2)\n",
    "    return rescale(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541f993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def brightness_reconstruction(img): # doi: 10.1109/TPS.2018.2828863.\n",
    "    im_norm = img / 255\n",
    "    im_ave = np.average(im_norm,axis=None)\n",
    "    significance = np.log(im_norm + 1) * (im_norm - im_ave)\n",
    "    probability = significance / np.max(significance)\n",
    "    fixed_probability = np.where(probability < 0, 0, probability)\n",
    "    return fixed_probability * 255\n",
    "\n",
    "def fourier_shifting(img):\n",
    "    dft = np.fft.fft2(img, axes=(0,1))\n",
    "    dft_shift = np.fft.fftshift(dft)\n",
    "    radius = 1\n",
    "    mask = np.zeros_like(img)\n",
    "    cy = mask.shape[0] // 2\n",
    "    cx = mask.shape[1] // 2\n",
    "    cv2.circle(mask, (cx,cy), radius, (255,255,255), -1)[0]\n",
    "    mask = 255 - mask\n",
    "    dft_shift_masked = np.multiply(dft_shift,mask) / 255\n",
    "    back_ishift = np.fft.ifftshift(dft_shift)\n",
    "    back_ishift_masked = np.fft.ifftshift(dft_shift_masked)\n",
    "    img_back = np.fft.ifft2(back_ishift, axes=(0,1))\n",
    "    img_filtered = np.fft.ifft2(back_ishift_masked, axes=(0,1))\n",
    "    img_back = np.abs(img_back).clip(0,255).astype(np.uint8)\n",
    "    img_filtered = np.abs(3*img_filtered).clip(0,255).astype(np.uint8)\n",
    "    return img_filtered\n",
    "\n",
    "def prob_to_edge(image, threshold):\n",
    "    ratio = np.amax(image) / 255\n",
    "    img8 = (image/ratio).astype('uint8')\n",
    "    edge_ = cv2.Canny(img8, threshold[0], threshold[1])\n",
    "    return edge_\n",
    "\n",
    "def dark_filter(img):\n",
    "    img = np.where(img < 5, 0, img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600ae487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(img8, kernel_size, sigma, threshold, erode_kernel):\n",
    "    blur_gray = cv2.GaussianBlur(img8,(kernel_size, kernel_size),0)\n",
    "    probability = brightness_reconstruction(blur_gray)\n",
    "    probability = np.array(dip.MatchedFiltersLineDetector2D(probability, sigma = sigma)) # 10.1109/42.34715\n",
    "    probability *= 255.0/probability.max()\n",
    "    probability = brightness_reconstruction(probability)\n",
    "    probability = np.where(probability < threshold, 0, 1).astype('uint8')\n",
    "    probability = cv2.erode(probability, np.ones((erode_kernel,erode_kernel), np.uint8), iterations=1)\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-community",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-jordan",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# [inverted,radii,elevation,frames,times,vid_frames,vid_times,vid] = _load_data('/scratch/gpfs/aj17/plasmatv_data/tv_images/emission_structure_pu_cam240perp_185821.sav')\n",
    "filepath = Path('tv_images')\n",
    "filename = 'emission_structure_pu_cam240perp_185821'\n",
    "fullfile = filename + '.sav'\n",
    "[inverted,radii,elevation,frames,times,vid_frames,vid_times,vid] = _load_data(filepath / fullfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-madagascar",
   "metadata": {},
   "source": [
    "### Detecting lines in raw image (lines correspond to XPR and Emission Front)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-intensity",
   "metadata": {},
   "outputs": [],
   "source": [
    "tid = 200\n",
    "img = np.sqrt(vid[tid]).copy()\n",
    "gray=(255-255*(img-np.min(img))/(np.max(img)-np.min(img))).astype('uint8')\n",
    "\n",
    "# reduce the noise using Gaussian filters\n",
    "kernel_size = 11 \n",
    "blur_gray = cv2.GaussianBlur(gray,(kernel_size, kernel_size),0)\n",
    "\n",
    "# Apply Canny edge detctor\n",
    "low_threshold = 10\n",
    "high_threshold = 20\n",
    "edges = cv2.Canny(blur_gray, low_threshold, high_threshold)\n",
    "\n",
    "# Apply Hough transform\n",
    "rho = 1  # This is the distance resolution in pixels of the Hough grid\n",
    "theta = np.pi / 180  # angular resolution in radians of the Hough grid\n",
    "threshold = 5  # minimum number of votes (intersections in Hough grid cell)\n",
    "min_line_length = 20  # minimum number of pixels making up a line\n",
    "max_line_gap = 10  # maximum gap in pixels between connectable line segments\n",
    "line_image = np.zeros((img.shape[0],img.shape[1],3))  # creating a blank to draw lines on\n",
    "\n",
    "\n",
    "lines = cv2.HoughLinesP(edges, rho, theta, threshold, np.array([]), min_line_length, max_line_gap) # The output \"lines\" is an array containing endpoints of detected line segments\n",
    "\n",
    "for line in lines:\n",
    "    for x1,y1,x2,y2 in line:\n",
    "        cv2.line(line_image,(x1,y1),(x2,y2),(255,0,0),5)\n",
    "\n",
    "        \n",
    "line_len=[]\n",
    "for line in lines:\n",
    "    for x1,y1,x2,y2 in line:\n",
    "        line_len.append(np.sqrt((x2-x1)**2+(y2-y1)**2))\n",
    "        \n",
    "# add the line_image as an extra layer on top of the original image\n",
    "lines_edges = cv2.addWeighted(cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR), 1, line_image, 0.5, 0,dtype =0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-morning",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(lines_edges,aspect='auto')\n",
    "plt.title('Final result')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecd21f8",
   "metadata": {},
   "source": [
    "## Filtering Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83b612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single image plotting\n",
    "idx = 100\n",
    "kernel_size = 5\n",
    "sigma = 1\n",
    "threshold = 4\n",
    "erode_kernel = 4\n",
    "aspect_num = 1/2\n",
    "\n",
    "img = np.sqrt(vid[idx]).copy() # [25:250,250:700]\n",
    "ratio = np.amax(img) / 255\n",
    "img8 = (img/ratio).astype('uint8')\n",
    "img8 = img8[0:240, 240:720]\n",
    "probability = process_image(img8, kernel_size, sigma, threshold, erode_kernel)\n",
    "\n",
    "fig, ax = plt.subplots(2)\n",
    "ax[0].imshow(probability, cmap='gray')\n",
    "ax[1].imshow(img8, cmap='gray')\n",
    "x_left, x_right = ax[0].get_xlim()\n",
    "y_low, y_high = ax[0].get_ylim()\n",
    "ax[0].set_aspect(abs((x_right-x_left)/(y_low-y_high))*aspect_num)\n",
    "ax[1].set_aspect(abs((x_right-x_left)/(y_low-y_high))*aspect_num)\n",
    "ax[0].set_title(f'2X BR + 2DMFLD, Frame {idx}')\n",
    "ax[1].set_title(f'Original, Frame {idx}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd98b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# animated plotting\n",
    "savepath = Path('process_videos') / f'{filename}_inversion.mp4'\n",
    "\n",
    "kernel_size = 5\n",
    "sigma = 1\n",
    "threshold = 4\n",
    "erode_kernel = 4\n",
    "aspect_num = 1/2\n",
    "\n",
    "fig, ax = plt.subplots(2)\n",
    "top = ax[0].imshow(probability, cmap='gray')\n",
    "bot = ax[1].imshow(img8, cmap='gray')\n",
    "title1 = ax[0].set_title(f'2X BR + 2DMFLD, Frame {0}')\n",
    "title2 = ax[1].set_title(f'Original, Frame {0}')\n",
    "\n",
    "x_left, x_right = ax[0].get_xlim()\n",
    "y_low, y_high = ax[0].get_ylim()\n",
    "ax[0].set_aspect(abs((x_right-x_left)/(y_low-y_high))*aspect_num)\n",
    "ax[1].set_aspect(abs((x_right-x_left)/(y_low-y_high))*aspect_num)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "def animate(i):\n",
    "    img = np.sqrt(vid[i]).copy() # [25:250,250:700]\n",
    "    ratio = np.amax(img) / 255\n",
    "    img8 = (img/ratio).astype('uint8')\n",
    "    img8 = img8[0:240, 240:720]\n",
    "    probability = process_image(img8, kernel_size, sigma, threshold, erode_kernel)\n",
    "    top.set_array(probability)\n",
    "    bot.set_array(img8)\n",
    "    title1.set_text(f'2X BR + 2DMFLD, Frame {i}')\n",
    "    title2.set_text(f'Original, Frame {i}')\n",
    "    return top, bot, title1, title2\n",
    "    \n",
    "writervideo = animation.FFMpegWriter(fps=30) \n",
    "ani = animation.FuncAnimation(fig, animate, frames=tqdm(range(len(vid))), blit=True)\n",
    "ani.save(savepath, writer=writervideo)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-greene",
   "metadata": {},
   "source": [
    "## Training a model to predict X point coordinates based on synthetic XPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255e2c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_idx = 1000\n",
    "end_idx = 1500\n",
    "\n",
    "num_train_idx = cutoff_idx\n",
    "num_val_idx = end_idx - cutoff_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-naples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pickle.load(open('/projects/EKOLEMEN/plasmatv_data/synthetic_data/synthetic_outs.pl','rb'))\n",
    "data = pickle.load(open('synthetic_outs.pl','rb'))\n",
    "X=np.int_(np.dstack([v for k,v in data['image'].items()]))\n",
    "y=np.dstack([v for k,v in data['RZ'].items()])\n",
    "\n",
    "rand_ind=np.random.permutation(X.shape[2])\n",
    "\n",
    "X_train = X[:,:,rand_ind[:cutoff_idx]]\n",
    "y_train = y[:,:,rand_ind[:cutoff_idx]]\n",
    "\n",
    "X_valid = X[:,:,rand_ind[cutoff_idx:end_idx]]\n",
    "y_valid = y[:,:,rand_ind[cutoff_idx:end_idx]]\n",
    "\n",
    "X_test = X[:,:,rand_ind[end_idx:]]\n",
    "y_test = y[:,:,rand_ind[end_idx:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-imperial",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(X_train[:,:,100])\n",
    "plt.title(\"Intended\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-trick",
   "metadata": {},
   "source": [
    "Task 1: Train a model to predict a single X point using XPR synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-forest",
   "metadata": {},
   "source": [
    "Task 2: Load the syntheitc data (synthetic_outs_2d_ver2.pl) for both XPR and Emission Front and train a model to detect both inner and outer X points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-variety",
   "metadata": {},
   "source": [
    "Task 3: Detect the XPR and Emission Front in the raw image and redo Task 1&2 but with the detected lines rather than synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4d3730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "init_lr = 0.001\n",
    "batch_size = 4\n",
    "epochs = 2\n",
    "\n",
    "# dummy dims\n",
    "input_dim = 4\n",
    "output_dim = 2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=input_dim, out_channels=20, kernel_size=(5,5))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        self.linear = nn.Linear(358, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6170302",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TVDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[2]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[:,:,index], self.y[:,:,index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4739644",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_dim, output_dim).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=init_lr)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# training history\n",
    "H = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"valid_loss\": [],\n",
    "    \"valid_acc\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de238019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to torch tensors\n",
    "X_train_d = torch.from_numpy(X_train).float()\n",
    "y_train_d = torch.from_numpy(y_train).float()\n",
    "X_valid_d = torch.from_numpy(X_valid).float()\n",
    "y_valid_d = torch.from_numpy(y_valid).float()\n",
    "X_test_d = torch.from_numpy(X_test).float()\n",
    "y_test_d = torch.from_numpy(y_test).float()\n",
    "\n",
    "# data load debugging\n",
    "dataset = TVDataset(X_train_d, y_train_d)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff9162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "    train_correct = 0\n",
    "    val_correct = 0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        \n",
    "        (inputs, labels) = (inputs.to(device), labels.to(device))\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        train_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "        \n",
    "    H[\"train_loss\"].append(total_train_loss / len(dataloader))\n",
    "    H[\"train_acc\"].append(train_correct / len(dataloader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
