{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-blood",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import signal\n",
    "import math\n",
    "from scipy.io import readsav\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-spray",
   "metadata": {},
   "source": [
    "### Functions for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_data(filename):\n",
    "    dat = readsav(filename)\n",
    "    emission = dat['emission_structure']\n",
    "    return emission[0]\n",
    "\n",
    "def _find_index(arr,val):\n",
    "    return np.argmin(abs(arr-val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-theme",
   "metadata": {},
   "source": [
    "### Functions for enhancing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-oriental",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(data):\n",
    "    mn = data.mean()\n",
    "    std = data.std()\n",
    "    return((data-mn)/std)\n",
    "\n",
    "def rescale(data):\n",
    "    return (data-data.min())/(data.max()-data.min())\n",
    "\n",
    "def quantfilt(src,thr=0.9):\n",
    "    filt = np.quantile(src,thr,axis=0)\n",
    "    out = np.where(src<filt,0,src)\n",
    "    return out\n",
    "\n",
    "# gaussian filtering\n",
    "def gaussblr(src,filt=(31, 3)):\n",
    "    src = (rescale(src)*255).astype('uint8')\n",
    "    out = cv2.GaussianBlur(src,filt,0)\n",
    "    return rescale(out)\n",
    "\n",
    "# mean filtering\n",
    "def meansub(src):\n",
    "    mn = np.mean(src,axis=1)[:,np.newaxis]\n",
    "    out = np.absolute(src - mn)\n",
    "    return rescale(out)\n",
    "\n",
    "# morphological filtering\n",
    "def morph(src):\n",
    "    src = (rescale(src)*255).astype('uint8')\n",
    "    se1 = cv2.getStructuringElement(cv2.MORPH_RECT, (4,4))\n",
    "    se2 = cv2.getStructuringElement(cv2.MORPH_RECT, (3,1))\n",
    "    mask = cv2.morphologyEx(src, cv2.MORPH_CLOSE, se1)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, se2)\n",
    "    return rescale(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-community",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-jordan",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# [inverted,radii,elevation,frames,times,vid_frames,vid_times,vid] = _load_data('/scratch/gpfs/aj17/plasmatv_data/tv_images/emission_structure_pu_cam240perp_185821.sav')\n",
    "\n",
    "[inverted,radii,elevation,frames,times,vid_frames,vid_times,vid] = _load_data('emission_structure_pu_cam240perp_185821.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-monte",
   "metadata": {},
   "source": [
    "### Try to find the X point and visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf623b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(times.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-cannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(ncols=3,nrows=1,figsize=(15,5))\n",
    "fid =1\n",
    "tid = _find_index(vid_times,times[fid]) #find frame id for camera image with t=times[fid]\n",
    "\n",
    "find_x = inverted[fid]\n",
    "find_x = find_x/7.5 # max value of inverted in whole dataset\n",
    "\n",
    "find_x = np.power(find_x,2)\n",
    "\n",
    "gaussblr_win = (11,11) # Gaussian blur filter size\n",
    "find_x =  gaussblr(find_x,gaussblr_win)\n",
    "find_x = meansub(find_x)    \n",
    "# Sxx_enhanced = morph(Sxx_enhanced)\n",
    "# Sxx_enhanced = meansub(Sxx_enhanced)\n",
    "find_x  = np.where(find_x<0.3,0,1) # Threshold of 0.3 to detect spot\n",
    "\n",
    "\n",
    "ax[0].pcolormesh(vid[fid][::-1,:])\n",
    "ax[0].set(title='raw image')\n",
    "ax[1].pcolormesh(radii[fid],elevation[fid],inverted[fid],shading='auto')\n",
    "ax[1].set(title='Inverted image',xlabel='radii',ylabel='elevation')\n",
    "ax[2].pcolormesh(radii[fid],elevation[fid],find_x,shading='auto')\n",
    "ax[2].set(title='Finding X points',xlabel='radii',ylabel='elevation')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf71bbbc",
   "metadata": {},
   "source": [
    "### Edge Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7911df",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = inverted[fid].copy()\n",
    "gray=(255-255*(img-np.min(img))/(np.max(img)-np.min(img))).astype('uint8')\n",
    "useHarrisDetector = False # False uses Shi-Tomasi Corner Detector\n",
    "corners = cv2.goodFeaturesToTrack(gray,3,0.01,10, useHarrisDetector=useHarrisDetector)\n",
    "corners = np.intp(corners)\n",
    "x = radii[fid][corners[:,0,0]]\n",
    "y = elevation[fid][corners[:,0,1]]\n",
    "print(np.column_stack((x,y)))\n",
    "plt.pcolormesh(radii[fid],elevation[fid],img,shading='auto')\n",
    "plt.scatter(x,y,color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-madagascar",
   "metadata": {},
   "source": [
    "### Detecting lines in raw image (lines correspond to XPR and Emission Front)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-intensity",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.sqrt(vid[tid]).copy()\n",
    "gray=(255-255*(img-np.min(img))/(np.max(img)-np.min(img))).astype('uint8')\n",
    "\n",
    "# reduce the noise using Gaussian filters\n",
    "kernel_size = 11 \n",
    "blur_gray = cv2.GaussianBlur(gray,(kernel_size, kernel_size),0)\n",
    "\n",
    "# Apply Canny edge detctor\n",
    "low_threshold = 10\n",
    "high_threshold = 20\n",
    "edges = cv2.Canny(blur_gray, low_threshold, high_threshold)\n",
    "\n",
    "# Apply Hough transform\n",
    "rho = 1  # This is the distance resolution in pixels of the Hough grid\n",
    "theta = np.pi / 180  # angular resolution in radians of the Hough grid\n",
    "threshold = 5  # minimum number of votes (intersections in Hough grid cell)\n",
    "min_line_length = 20  # minimum number of pixels making up a line\n",
    "max_line_gap = 10  # maximum gap in pixels between connectable line segments\n",
    "line_image = np.zeros((img.shape[0],img.shape[1],3))  # creating a blank to draw lines on\n",
    "\n",
    "\n",
    "lines = cv2.HoughLinesP(edges, rho, theta, threshold, np.array([]), min_line_length, max_line_gap) # The output \"lines\" is an array containing endpoints of detected line segments\n",
    "\n",
    "for line in lines:\n",
    "    for x1,y1,x2,y2 in line:\n",
    "        cv2.line(line_image,(x1,y1),(x2,y2),(255,0,0),5)\n",
    "\n",
    "        \n",
    "line_len=[]\n",
    "for line in lines:\n",
    "    for x1,y1,x2,y2 in line:\n",
    "        line_len.append(np.sqrt((x2-x1)**2+(y2-y1)**2))\n",
    "        \n",
    "# add the line_image as an extra layer on top of the original image\n",
    "lines_edges = cv2.addWeighted(cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR), 1, line_image, 0.5, 0,dtype =0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-morning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots\n",
    "fig,ax = plt.subplots(ncols=3,nrows=2,figsize=(30,10))\n",
    "\n",
    "\n",
    "ax[0,0].imshow(gray,cmap='gray',aspect='auto')\n",
    "ax[0,0].set(title='Image')\n",
    "\n",
    "ax[0,1].imshow(blur_gray,cmap='gray',aspect='auto')\n",
    "ax[0,1].set(title='Smoothened')\n",
    "\n",
    "ax[0,2].imshow(edges,cmap='gray',aspect='auto')\n",
    "ax[0,2].set (title='Canny Edge Detection')\n",
    "\n",
    "ax[1, 0].imshow(line_image,cmap='gray',aspect='auto')\n",
    "ax[1, 0].set(title='Hough transform')\n",
    "\n",
    "ax[1, 1].imshow(lines_edges,aspect='auto')\n",
    "ax[1, 1].set(title='Final result')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecd21f8",
   "metadata": {},
   "source": [
    "## Other Filtering Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f31c964",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.sqrt(vid[tid]).copy() # [25:250,250:700]\n",
    "ratio = np.amax(img) / 255\n",
    "img8 = (img/ratio).astype('uint8')\n",
    "kernel_size = 5\n",
    "blur_gray = cv2.GaussianBlur(img8,(kernel_size, kernel_size),0)\n",
    "\n",
    "# img = cv2.imread('zhang4ab-2828863-large.jpg')\n",
    "# img8 = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)[100:200,100:200]\n",
    "\n",
    "plt.imshow(img8, cmap='gray')\n",
    "plt.title('Original')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54db245",
   "metadata": {},
   "source": [
    "### Fourier Shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff13688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do dft saving as complex output\n",
    "dft = np.fft.fft2(blur_gray, axes=(0,1))\n",
    "\n",
    "# apply shift of origin to center of image\n",
    "dft_shift = np.fft.fftshift(dft)\n",
    "\n",
    "# generate spectrum from magnitude image (for viewing only)\n",
    "mag = np.abs(dft_shift)\n",
    "spec = np.log(mag) / 20\n",
    "\n",
    "# create white circle mask on black background and invert so black circle on white background\n",
    "radius = 1\n",
    "mask = np.zeros_like(img8)\n",
    "cy = mask.shape[0] // 2\n",
    "cx = mask.shape[1] // 2\n",
    "cv2.circle(mask, (cx,cy), radius, (255,255,255), -1)[0]\n",
    "mask = 255 - mask\n",
    "\n",
    "# blur the mask\n",
    "mask2 = cv2.GaussianBlur(mask, (19,19), 0)\n",
    "\n",
    "# apply mask to dft_shift\n",
    "dft_shift_masked = np.multiply(dft_shift,mask) / 255\n",
    "dft_shift_masked2 = np.multiply(dft_shift,mask2) / 255\n",
    "\n",
    "\n",
    "# shift origin from center to upper left corner\n",
    "back_ishift = np.fft.ifftshift(dft_shift)\n",
    "back_ishift_masked = np.fft.ifftshift(dft_shift_masked)\n",
    "back_ishift_masked2 = np.fft.ifftshift(dft_shift_masked2)\n",
    "\n",
    "\n",
    "# do idft saving as complex output\n",
    "img_back = np.fft.ifft2(back_ishift, axes=(0,1))\n",
    "img_filtered = np.fft.ifft2(back_ishift_masked, axes=(0,1))\n",
    "img_filtered2 = np.fft.ifft2(back_ishift_masked2, axes=(0,1))\n",
    "\n",
    "# combine complex real and imaginary components to form (the magnitude for) the original image again\n",
    "# multiply by 3 to increase brightness\n",
    "img_back = np.abs(img_back).clip(0,255).astype(np.uint8)\n",
    "img_filtered = np.abs(3*img_filtered).clip(0,255).astype(np.uint8)\n",
    "img_filtered2 = np.abs(3*img_filtered2).clip(0,255).astype(np.uint8)\n",
    "\n",
    "plt.imshow(img_filtered, cmap='gray')\n",
    "plt.title('FFT Filtering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1551e53",
   "metadata": {},
   "source": [
    "### Brightness Reconstruction\n",
    "doi: 10.1109/TPS.2018.2828863."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541f993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability(img):\n",
    "    \n",
    "    im_norm = img / 255\n",
    "    im_ave = np.average(im_norm)\n",
    "    significance = np.log(im_norm+1) * (im_norm - im_ave)\n",
    "    probability = significance / np.max(significance)\n",
    "    fixed_probability = np.where(probability < 0, 0, probability)\n",
    "            \n",
    "    return fixed_probability * 255\n",
    "\n",
    "def prob_to_edge(image, threshold):\n",
    "    ratio = np.amax(image) / 255\n",
    "    img8 = (image/ratio).astype('uint8')\n",
    "    edge_ = cv2.Canny(img8, threshold[0], threshold[1])\n",
    "    \n",
    "    \n",
    "    # ksize = 5\n",
    "    # sobelx = cv2.Sobel(image,cv2.CV_64F,1,0,ksize=ksize)\n",
    "    # sobely = cv2.Sobel(image,cv2.CV_64F,0,1,ksize=ksize)\n",
    "    # edge = np.sqrt(sobelx**2 + sobely**2)\n",
    "    \n",
    "    # gx, gy = np.gradient(image)\n",
    "    # g = np.sqrt(gx**2 + gy**2)\n",
    "    # dim = g.shape\n",
    "    # edge = np.zeros_like(image)\n",
    "    # for i in range(dim[0]):\n",
    "    #     for j in range(dim[1]):\n",
    "    #         edge[i,j] = 255 * (g[i,j] == 0)\n",
    "            \n",
    "    return edge_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8103a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge = prob_to_edge(img_filtered,[65,65])\n",
    "plt.imshow(edge, cmap='gray')\n",
    "plt.title('FFT Filtering + Canny')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83b612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "probability = get_probability(blur_gray)\n",
    "plt.imshow(probability, cmap='gray')\n",
    "plt.title('Brightness Reconstruction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5f53d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge2 = prob_to_edge(probability,[70,85])\n",
    "plt.imshow(edge2, cmap='gray')\n",
    "plt.title('Brightness Reconstruction + Canny')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e05ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = 1  # This is the distance resolution in pixels of the Hough grid\n",
    "theta = np.pi / 180  # angular resolution in radians of the Hough grid\n",
    "threshold = 5  # minimum number of votes (intersections in Hough grid cell)\n",
    "min_line_length = 20  # minimum number of pixels making up a line\n",
    "max_line_gap = 10  # maximum gap in pixels between connectable line segments\n",
    "line_image = np.zeros((img.shape[0],img.shape[1],3))  # creating a blank to draw lines on\n",
    "\n",
    "\n",
    "lines = cv2.HoughLinesP(edge, rho, theta, threshold, np.array([]), min_line_length, max_line_gap) # The output \"lines\" is an array containing endpoints of detected line segments\n",
    "\n",
    "for line in lines:\n",
    "    for x1,y1,x2,y2 in line:\n",
    "        cv2.line(line_image,(x1,y1),(x2,y2),(255,0,0),5)\n",
    "\n",
    "        \n",
    "line_len=[]\n",
    "for line in lines:\n",
    "    for x1,y1,x2,y2 in line:\n",
    "        line_len.append(np.sqrt((x2-x1)**2+(y2-y1)**2))\n",
    "        \n",
    "# add the line_image as an extra layer on top of the original image\n",
    "lines_edges = cv2.addWeighted(cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR), 1, line_image, 0.5, 0,dtype =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c12d537",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(lines_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384df80c",
   "metadata": {},
   "source": [
    "### Significance Tiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aa8f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually test image size for tiling\n",
    "print(blur_gray.shape)\n",
    "print(blur_gray.shape[0] % 24, blur_gray.shape[1] % 72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7565d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_split(img, kernel_size):\n",
    "    img_height, img_width = img.shape\n",
    "    tile_height, tile_width = kernel_size\n",
    "    \n",
    "    num_rows = img_height // tile_height\n",
    "    num_cols = img_width // tile_width\n",
    "    \n",
    "    img = img.reshape(num_rows, tile_height, num_cols,tile_width)\n",
    "    img = img.swapaxes(1,2).reshape(-1, tile_height, tile_width)\n",
    "\n",
    "    return img\n",
    "\n",
    "def recombine_tiles(img, img_shape, kernel_size):\n",
    "    img_height, img_width = img_shape\n",
    "    tile_height, tile_width = kernel_size\n",
    "    \n",
    "    num_rows = img_height // tile_height\n",
    "    num_cols = img_width // tile_width\n",
    "    \n",
    "    img = img.reshape(num_rows, num_cols, tile_height, tile_width)\n",
    "    img = img.swapaxes(1, 2).reshape(img_height, img_width)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62843c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_tiles = img_split(blur_gray, (24, 72))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f896942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_tiles(img, kernel_size):\n",
    "    \n",
    "    tile_im = img_split(img, kernel_size)\n",
    "    \n",
    "    steps = tile_im.shape[0]\n",
    "    \n",
    "    for i in range(steps):\n",
    "        tile_im[i] = get_probability(tile_im[i])\n",
    "        \n",
    "    combined_im = recombine_tiles(tile_im, img.shape, kernel_size)\n",
    "    \n",
    "    return combined_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83ec05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(prob_tiles(blur_gray, (48, 120)), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-greene",
   "metadata": {},
   "source": [
    "## Training a model to predict X point coordinates based on synthetic XPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255e2c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_idx = 1000\n",
    "end_idx = 1500\n",
    "\n",
    "num_train_idx = cutoff_idx\n",
    "num_val_idx = end_idx - cutoff_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-naples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pickle.load(open('/projects/EKOLEMEN/plasmatv_data/synthetic_data/synthetic_outs.pl','rb'))\n",
    "data = pickle.load(open('synthetic_outs.pl','rb'))\n",
    "X=np.int_(np.dstack([v for k,v in data['image'].items()]))\n",
    "y=np.dstack([v for k,v in data['RZ'].items()])\n",
    "\n",
    "rand_ind=np.random.permutation(X.shape[2])\n",
    "\n",
    "X_train = X[:,:,rand_ind[:cutoff_idx]]\n",
    "y_train = y[:,:,rand_ind[:cutoff_idx]]\n",
    "\n",
    "X_valid = X[:,:,rand_ind[cutoff_idx:end_idx]]\n",
    "y_valid = y[:,:,rand_ind[cutoff_idx:end_idx]]\n",
    "\n",
    "X_test = X[:,:,rand_ind[end_idx:]]\n",
    "y_test = y[:,:,rand_ind[end_idx:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-imperial",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolormesh(X_train[:,:,100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-trick",
   "metadata": {},
   "source": [
    "Task 1: Train a model to predict a single X point using XPR synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-forest",
   "metadata": {},
   "source": [
    "Task 2: Load the syntheitc data (synthetic_outs_2d_ver2.pl) for both XPR and Emission Front and train a model to detect both inner and outer X points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-variety",
   "metadata": {},
   "source": [
    "Task 3: Detect the XPR and Emission Front in the raw image and redo Task 1&2 but with the detected lines rather than synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4d3730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "init_lr = 0.001\n",
    "batch_size = 4\n",
    "epochs = 2\n",
    "\n",
    "# dummy dims\n",
    "input_dim = 4\n",
    "output_dim = 2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-boundary",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=input_dim, out_channels=20, kernel_size=(5,5))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        self.linear = nn.Linear(358, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6170302",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TVDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[2]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[:,:,index], self.y[:,:,index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4739644",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_dim, output_dim).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=init_lr)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# training history\n",
    "H = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"valid_loss\": [],\n",
    "    \"valid_acc\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de238019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to torch tensors\n",
    "X_train_d = torch.from_numpy(X_train).float()\n",
    "y_train_d = torch.from_numpy(y_train).float()\n",
    "X_valid_d = torch.from_numpy(X_valid).float()\n",
    "y_valid_d = torch.from_numpy(y_valid).float()\n",
    "X_test_d = torch.from_numpy(X_test).float()\n",
    "y_test_d = torch.from_numpy(y_test).float()\n",
    "\n",
    "# data load debugging\n",
    "dataset = TVDataset(X_train_d, y_train_d)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff9162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "    train_correct = 0\n",
    "    val_correct = 0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        \n",
    "        (inputs, labels) = (inputs.to(device), labels.to(device))\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        train_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "        \n",
    "    H[\"train_loss\"].append(total_train_loss / len(dataloader))\n",
    "    H[\"train_acc\"].append(train_correct / len(dataloader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
