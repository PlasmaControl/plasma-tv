{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from scipy.io import readsav\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_data(filename):\n",
    "    dat = readsav(filename)\n",
    "    emission = dat['emission_structure']\n",
    "    return emission[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = Path('tv_images')\n",
    "label_path = Path('inversion_data')\n",
    "\n",
    "target_names = [f for f in target_path.glob('*') if f.is_file()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Point Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_idx = 0\n",
    "data_idx = 0\n",
    "# inverted, radii, elevation, frames, times, vid_frames, vid_times, vid\n",
    "[inverted,_,_,frames,_,_,_,vid] = _load_data(target_names[file_idx])\n",
    "# frame, x_location, l_location, r_location, x_intensity, l_intensity, r_intensity\n",
    "pkl_path = (label_path / target_names[file_idx].stem).with_suffix('.pkl') # target and label have same name stem\n",
    "pkl_file = open(pkl_path, 'rb')\n",
    "label_info = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "invert_idx = frames.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_point = label_info['l_location'][data_idx]\n",
    "r_point = label_info['r_location'][data_idx]\n",
    "print(f\"left point = {l_point}\")\n",
    "print(f\"right point = {r_point}\")\n",
    "# test thing for github\n",
    "label_norm = inverted[0].shape[0]\n",
    "target_norm = 255\n",
    "print(f\"label normalization = {label_norm}\")\n",
    "\n",
    "plt.imshow(inverted[data_idx], cmap = 'plasma')\n",
    "plt.scatter(l_point[0],l_point[1],c='lime',s=1)\n",
    "plt.scatter(r_point[0],r_point[1],c='lime',s=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_point = vid[invert_idx[data_idx]] / target_norm\n",
    "label_point = np.array([label_info['l_location'][data_idx], label_info['r_location'][data_idx]]).ravel() / label_norm\n",
    "print(f\"label = {label_point}\")\n",
    "\n",
    "im_ratio = target_point.shape[1]/target_point.shape[0]\n",
    "plt.imshow(target_point, vmin=0, vmax=1, cmap = 'plasma')\n",
    "plt.colorbar(orientation=\"horizontal\",fraction=0.047*im_ratio)\n",
    "plt.title(\"target\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloading Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TVDataset(Dataset):\n",
    "    def __init__(self, target_path, label_path, file_name):\n",
    "        self.target_path = target_path\n",
    "        self.label_path = label_path\n",
    "        self.file_name = file_name\n",
    "        self.pkl_path = (self.label_path / self.file_name.stem).with_suffix('.pkl')\n",
    "        self.target_norm = 255\n",
    "        self.label_norm = 201\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(readsav(self.file_name)['emission_structure'][0][3]) # gets length of frames\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        frame = readsav(self.file_name)['emission_structure'][0][3][idx]\n",
    "        target = readsav(self.file_name)['emission_structure'][0][7][int(frame)] / self.target_norm\n",
    "        with open(self.pkl_path, 'rb') as pkl_file:\n",
    "            label_info = pickle.load(pkl_file)\n",
    "        label = np.array([label_info['l_location'][idx], label_info['r_location'][idx]]).ravel() / self.label_norm\n",
    "        return target, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TVDataset(target_path, label_path, target_names[file_idx])\n",
    "print(f'label = {test_dataset[0][1]}')\n",
    "plt.imshow(test_dataset[0][0], vmin=0, vmax=1, cmap = 'plasma')\n",
    "plt.colorbar(orientation=\"horizontal\",fraction=0.047*im_ratio)\n",
    "plt.title(\"target\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(test_dataset, batch_size=4,\n",
    "                        shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    print(i_batch, sample_batched[0].size(),\n",
    "          sample_batched[1].size())\n",
    "    if i_batch == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeuralNet(nn.Module):\n",
    "\t#  Determine what layers and their order in CNN object \n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNeuralNet, self).__init__()\n",
    "        self.conv_layer1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)\n",
    "        self.conv_layer2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        \n",
    "        self.conv_layer3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.conv_layer4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3)\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(1600, 4)\n",
    "    \n",
    "    # Progresses data across layers    \n",
    "    def forward(self, x):\n",
    "        out = self.conv_layer1(x)\n",
    "        out = self.conv_layer2(out)\n",
    "        out = self.max_pool1(out)\n",
    "        \n",
    "        out = self.conv_layer3(out)\n",
    "        out = self.conv_layer4(out)\n",
    "        out = self.max_pool2(out)\n",
    "                \n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNeuralNet(num_classes=4)\n",
    "n_epochs = 10\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
