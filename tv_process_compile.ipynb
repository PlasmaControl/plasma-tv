{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile Data into HDF5 File\n",
    "Loads given data into a single HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import resize\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from utils.get_file import GetTV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inversion_data_path = Path('outputs/inversion_data')\n",
    "hdf5_path = Path('outputs/hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = GetTV('tv_images/l-mode')\n",
    "files = tv.list_files()\n",
    "file_lengths = [tv.file_len(f, False) for f in files]\n",
    "cumulative_lengths = np.insert(np.cumsum(file_lengths), 0, 0)\n",
    "tv_dim = tv.load(files[0], 'vid').shape\n",
    "inversion_dim = tv.load(files[0], 'inverted').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cumulative_lengths)\n",
    "print(tv_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw and Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file_name = hdf5_path / 'tv_raw.hdf5'\n",
    "\n",
    "with h5py.File(hdf5_file_name, 'w') as hf:\n",
    "    tv_dataset = hf.create_dataset(\"tv_images\", shape=(np.sum(file_lengths), tv_dim[0], tv_dim[1]), dtype='uint8')\n",
    "    points_dataset = hf.create_dataset(\"points\", shape=(np.sum(file_lengths), 4), dtype='float32')\n",
    "    for idx, file in enumerate(files):\n",
    "        frames = tv._load_data(file, 'frames')\n",
    "        tv_image = tv._load_data(file, 'vid')\n",
    "        tv_image_process = np.asarray(tv_image) # faster process and convert to binary\n",
    "        \n",
    "        pkl_path = (inversion_data_path / file.stem).with_suffix('.pkl')\n",
    "        with open(pkl_path, 'rb') as pkl_file:\n",
    "            label_info = pickle.load(pkl_file)\n",
    "        points = np.concatenate((label_info['l_location'], label_info['r_location']),1)\n",
    "        \n",
    "        for i in range(file_lengths[idx]):\n",
    "            tv_dataset[cumulative_lengths[idx]+i] = tv_image_process[i]\n",
    "            points_dataset[cumulative_lengths[idx]+i] = points[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inversion and Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With auto-labeled points\n",
    "\n",
    "hdf5_file_name = hdf5_path / 'compiled_inversion_no_image.hdf5'\n",
    "hf = h5py.File(hdf5_file_name, 'w') # open h5py file\n",
    "rz_dataset = hf.create_dataset(\"rz\", shape=(np.sum(file_lengths), 4), dtype='float32')\n",
    "intensity_dataset = hf.create_dataset(\"intensity\", shape=(np.sum(file_lengths), 2), dtype='float32')\n",
    "\n",
    "# Add datasets to the groups\n",
    "for idx, file in enumerate(files):\n",
    "    pkl_path = (inversion_data_path / file.stem).with_suffix('.pkl')\n",
    "    with open(pkl_path, 'rb') as pkl_file:\n",
    "            label_info = pickle.load(pkl_file)\n",
    "    points = np.concatenate((label_info['l_location'], label_info['r_location']),1)\n",
    "    points_i = np.concatenate((label_info['l_intensity'], label_info['r_intensity']))\n",
    "\n",
    "    for i in range(file_lengths[idx]):\n",
    "            rz_dataset[cumulative_lengths[idx]+i] = points[i]\n",
    "            intensity_dataset[cumulative_lengths[idx]+i] = points_i[i]\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With manual points\n",
    "csv_location = Path('outputs/manual_labeled_points')\n",
    "tv_location = Path('tv_images/l-mode-train')\n",
    "hdf5_file_name = hdf5_path / 'inversion_manual.hdf5'\n",
    "\n",
    "tv = GetTV(tv_location)\n",
    "csvs = GetTV(csv_location)\n",
    "csv_files = csvs.list_files()\n",
    "tv_files = tv.list_files()\n",
    "print(tv_files)\n",
    "csv_lens  = []\n",
    "for file in csv_files:\n",
    "    with open(file) as object:\n",
    "        csv_lens.append(sum(1 for line in object)-1)\n",
    "csv_len = sum(csv_lens)\n",
    "inverted_dim = tv.load(tv_files[0], 'inverted').shape\n",
    "tv_file = tv_files[0]\n",
    "csv_file = csv_files[0]\n",
    "cumulative_lengths = np.insert(np.cumsum(csv_lens), 0, 0)\n",
    "f_open = pd.read_csv(csv_file).to_numpy()\n",
    "with h5py.File(hdf5_file_name, 'w') as hf:\n",
    "    \n",
    "    tv_dataset = hf.create_dataset(\"inverted\", shape=(csv_len, inverted_dim[1], inverted_dim[2]), dtype='float32')\n",
    "    points_dataset = hf.create_dataset(\"points\", shape=(csv_len, 2), dtype='float32')\n",
    "    for idx, file in enumerate(tv_files):\n",
    "        inverted = tv.load(file, 'inverted')\n",
    "        process = np.asarray(inverted) # faster process and convert to binary\n",
    "        f_open = pd.read_csv(csv_files[idx]).to_numpy()\n",
    "        for i in range(csv_lens[idx]):\n",
    "            tv_dataset[cumulative_lengths[idx]+i] = process[i]\n",
    "            l1, l2 = int(f_open[i][1])/4, int(f_open[i][2])/4\n",
    "            points_dataset[cumulative_lengths[idx]+i] = [l1, l2]\n",
    "            \n",
    "# with open(csv_file, mode ='r')as file:\n",
    "#     csvFile = csv.reader(file)\n",
    "#     for lines in csvFile:\n",
    "#         plt.imshow(tv.load(tv_file, 'inverted')[0])\n",
    "#         plt.scatter(int(lines[1])/4, int(lines[2])/4, c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw and Synthetic and Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file_name = hdf5_path / 'x_outer_radiation.hdf5'\n",
    "hf = h5py.File(hdf5_file_name, 'w') # open h5py file\n",
    "\n",
    "# Add datasets to the groups\n",
    "for idx, file in enumerate(files):\n",
    "    frames = tv._load_data(file, 'frames')\n",
    "    tv_image = tv._load_data(file, 'vid_frames')\n",
    "    tv_image_process = np.asarray(tv_image) # faster process and convert to binary\n",
    "    pkl_path = (inversion_data_path / file.stem).with_suffix('.pkl')\n",
    "    with open(pkl_path, 'rb') as pkl_file:\n",
    "            label_info = pickle.load(pkl_file)\n",
    "    points = np.concatenate((label_info['x_location'], label_info['r_location']),1)\n",
    "    points_i = np.concatenate((label_info['x_intensity'], label_info['r_intensity']))\n",
    "    \n",
    "    for i in range(file_lengths[idx]):\n",
    "        tv_dataset[cumulative_lengths[idx]+i] = tv_image_process[i]\n",
    "        points_dataset[cumulative_lengths[idx]+i] = points[i]\n",
    "        intensity_dataset[cumulative_lengths[idx]+i] = points_i[i]\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw and Inversion and Points for Direct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Point Model\n",
    "modelpath = Path('outputs/models')\n",
    "file_name = 'lr_inversion_manual.pkl'\n",
    "with open(modelpath / file_name, 'rb') as f:\n",
    "    inversion_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# homogenous data\n",
    "points_train_test = []\n",
    "tv_train_test = []\n",
    "inverted_train_test = []\n",
    "for idx, file in enumerate(files):\n",
    "    frames = tv.load(file, 'frames').astype('int')\n",
    "    tv_image = tv.load(file, 'vid')[frames]\n",
    "    inversion = tv.load(file, 'inverted')\n",
    "    inversion_vid2 = inversion.reshape((len(inversion), -1))\n",
    "    for i in range(len(frames)):\n",
    "        tv_train_test.append(tv_image[i])\n",
    "        inverted_train_test.append(inversion[i])\n",
    "        points_train_test.append(inversion_model.predict(inversion_vid2[i].reshape(1, -1)))\n",
    "        \n",
    "print(np.array(points_train_test).shape)\n",
    "print(np.array(tv_train_test).shape)\n",
    "print(np.array(inverted_train_test).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_only = []\n",
    "for idx, file in enumerate(files):\n",
    "    frames = np.setdiff1d(tv.load(file, 'vid_frames').astype(int), tv.load(file, 'frames').astype(int))\n",
    "    tv_image = tv.load(file, 'vid')[frames]\n",
    "    for i in range(len(frames)):\n",
    "        tv_only.append(tv_image[i])\n",
    "        \n",
    "print(np.array(tv_only).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(hdf5_path / 'tv_inv_outer.h5', 'w') as f:\n",
    "    f.create_dataset('vid', data=tv_train_test)\n",
    "    f.create_dataset('inverted', data=inverted_train_test)\n",
    "    f.create_dataset('points', data=points_train_test)\n",
    "    f.create_dataset('vid_only', data=tv_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split by file\n",
    "train_files, test_files = train_test_split(files, test_size=0.2, random_state=30)\n",
    "print(train_files)\n",
    "print(test_files)\n",
    "\n",
    "points_train = []\n",
    "tv_train = []\n",
    "inverted_train = []\n",
    "points_test = []\n",
    "tv_test = []\n",
    "inverted_test = []\n",
    "\n",
    "for idx, file in enumerate(train_files):\n",
    "    frames = tv.load(file, 'frames').astype('int')\n",
    "    tv_image = tv.load(file, 'vid')[frames]\n",
    "    inversion = tv.load(file, 'inverted')\n",
    "    inversion_vid2 = inversion.reshape((len(inversion), -1))\n",
    "    for i in range(len(frames)):\n",
    "        tv_train.append(tv_image[i])\n",
    "        inverted_train.append(inversion[i])\n",
    "        points_train.append(inversion_model.predict(inversion_vid2[i].reshape(1, -1)))\n",
    "        \n",
    "for idx, file in enumerate(test_files):\n",
    "    frames = tv.load(file, 'frames').astype('int')\n",
    "    tv_image = tv.load(file, 'vid')[frames]\n",
    "    inversion = tv.load(file, 'inverted')\n",
    "    inversion_vid2 = inversion.reshape((len(inversion), -1))\n",
    "    for i in range(len(frames)):\n",
    "        tv_test.append(tv_image[i])\n",
    "        inverted_test.append(inversion[i])\n",
    "        points_test.append(inversion_model.predict(inversion_vid2[i].reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(hdf5_path / 'tv_inv_outer.h5', 'w') as f:\n",
    "    f.create_dataset('vid_train', data=tv_train)\n",
    "    f.create_dataset('inverted_train', data=inverted_train)\n",
    "    f.create_dataset('points_train', data=points_train)\n",
    "    f.create_dataset('vid_test', data=tv_test)\n",
    "    f.create_dataset('inverted_test', data=inverted_test)\n",
    "    f.create_dataset('points_test', data=points_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw and Inversion and Points for Cycle-GAN\n",
    "\n",
    "Similar to direct, but normalizes images and changes resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_train_test = []\n",
    "inverted_train_test = []\n",
    "for idx, file in tqdm(enumerate(files)):\n",
    "    frames = tv.load(file, 'frames').astype('int')\n",
    "    tv_image = (tv.load(file, 'vid')[frames] - 127.5)/127.5\n",
    "    inversion = (tv.load(file, 'inverted') - 7.5)/7.5\n",
    "    \n",
    "    tv_image = np.flip(resize(tv_image, (len(frames), 256, 256), order=0, preserve_range=True), axis=(2,1))\n",
    "    inversion = resize(inversion, (len(frames), 256, 256), order=0, preserve_range=True)\n",
    "    \n",
    "    for i in range(len(frames)):\n",
    "        tv_train_test.append(tv_image[i])\n",
    "        inverted_train_test.append(inversion[i])\n",
    "        \n",
    "print(np.array(tv_train_test).shape)\n",
    "print(np.array(inverted_train_test).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tv_only = []\n",
    "# for idx, file in tqdm(enumerate(files)):\n",
    "#     frames = np.setdiff1d(tv.load(file, 'vid_frames').astype(int), tv.load(file, 'frames').astype(int))\n",
    "#     tv_image = (tv.load(file, 'vid')[frames] - 127.5)/127.5\n",
    "    \n",
    "#     tv_image = np.flip(resize(tv_image, (len(frames), 256, 256), order=0, preserve_range=True), axis=(2,1))\n",
    "    \n",
    "#     for i in range(len(frames)):\n",
    "#         tv_only.append(tv_image[i])\n",
    "        \n",
    "# print(np.array(tv_only).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2000\n",
    "print(np.max(tv_train_test), np.min(tv_train_test))\n",
    "print(np.max(inverted_train_test[idx]), np.min(inverted_train_test[idx]))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(tv_train_test[idx], origin='lower')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(inverted_train_test[idx], origin='lower')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tv_train_test, inverted_train_test, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tv_train_test, inverted_train_test, test_size=0.2, random_state=42)\n",
    "with h5py.File(hdf5_path / 'img_2_img_proof.h5', 'w') as f:\n",
    "    f.create_dataset('A_train', data=X_train)\n",
    "    f.create_dataset('A_test', data=X_test)\n",
    "    f.create_dataset('B_train', data=y_train)\n",
    "    f.create_dataset('B_test', data=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full Training Set and Full TV Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(hdf5_path / 'img_2_img_full_train.h5', 'w') as f:\n",
    "    f.create_dataset('A_train', data=tv_train_test)\n",
    "    f.create_dataset('B_train', data=inverted_train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_only = []\n",
    "for idx, file in tqdm(enumerate(files)):\n",
    "    frames = tv.load(file, 'vid_frames').astype(int)\n",
    "    tv_image = (tv.load(file, 'vid') - 127.5)/127.5\n",
    "    \n",
    "    tv_image = np.flip(resize(tv_image, (len(frames), 256, 256), order=0, preserve_range=True), axis=(2,1))\n",
    "    \n",
    "    for i in range(len(frames)):\n",
    "        tv_only.append(tv_image[i])\n",
    "        \n",
    "print(np.array(tv_only).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(hdf5_path / 'img_2_img_full_tv.h5', 'w') as f:\n",
    "    f.create_dataset('A_test', data=tv_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TV Synthetic HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomization = False\n",
    "\n",
    "file_name_1 = Path('outputs/hdf5/s_outs_v3_limited.h5')\n",
    "file_name_2 = Path('outputs/hdf5/x_outer_radiation.hdf5')\n",
    "\n",
    "out_path = Path('outputs')\n",
    "\n",
    "with h5py.File(file_name_1, 'r') as f:\n",
    "    synthetic_images = f['image'][:] * 2 - 1\n",
    "    \n",
    "with h5py.File(file_name_2, 'r') as f:\n",
    "    points = f['points'][:]\n",
    "    tv_images = f['tv_images'][:] / 127.5 - 1\n",
    "\n",
    "print(len(synthetic_images), len(tv_images))\n",
    "file_len = 1840\n",
    "\n",
    "crop_synthetic = resize(synthetic_images[:file_len], (file_len, 256, 256))\n",
    "crop_tv = np.flip(resize(tv_images[:file_len], (file_len, 256, 256)), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(synthetic_images.min(), synthetic_images.max())\n",
    "print(tv_images.min(), tv_images.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1799\n",
    "axs1 = plt.subplot(1,2,1)\n",
    "axs1.imshow(crop_synthetic[idx], origin='lower')\n",
    "print(np.min(crop_synthetic[idx]), np.max(crop_synthetic[idx]))\n",
    "\n",
    "axs2 = plt.subplot(1,2,2)\n",
    "axs2.imshow(crop_tv[idx], origin='lower')\n",
    "print(np.min(crop_tv[idx]), np.max(crop_tv[idx]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(synth_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tts_percent = 0.99\n",
    "tvs_percent = 0.8\n",
    "\n",
    "# synth_dat, synth_val, tv_dat, tv_val = train_test_split(crop_synthetic, crop_tv, train_size=tts_percent, random_state=42)\n",
    "synth_train, synth_test, tv_train, tv_test = train_test_split(crop_synthetic, crop_tv, train_size=tvs_percent, random_state=42)\n",
    "\n",
    "# print(len(synth_dat), len(synth_val), len(tv_dat), len(tv_val))\n",
    "print(len(synth_train), len(synth_test), len(tv_train), len(tv_test))\n",
    "with h5py.File(out_path / 'tv_synth.h5', 'w') as f:\n",
    "    f.create_dataset('synth_train', data=synth_train)\n",
    "    f.create_dataset('synth_test', data=synth_test)\n",
    "    # f.create_dataset('synth_val', data=synth_val)\n",
    "    f.create_dataset('tv_train', data=tv_train)\n",
    "    f.create_dataset('tv_test', data=tv_test)\n",
    "    # f.create_dataset('tv_val', data=tv_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axs1 = plt.subplot(1,2,1)\n",
    "axs1.imshow(crop_tv[211], origin='lower')\n",
    "axs2 = plt.subplot(1,2,2)\n",
    "axs2.imshow(crop_synthetic[10], origin='lower')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'outputs/tv_synth.h5'\n",
    "\n",
    "with h5py.File(file_name, 'r') as f:\n",
    "    synth_train = f['synth_train'][:]\n",
    "    synth_test = f['synth_test'][:]\n",
    "    # synth_val = f['synth_val'][:]\n",
    "    tv_train = f['tv_train'][:]\n",
    "    tv_test = f['tv_test'][:]\n",
    "    # tv_val = f['tv_val'][:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 884\n",
    "axs1 = plt.subplot(1,2,1)\n",
    "axs1.imshow(synth_train[idx], origin='lower')\n",
    "# print(np.min(synth_val[idx]), np.max(synth_val[idx]))\n",
    "\n",
    "axs2 = plt.subplot(1,2,2)\n",
    "axs2.imshow(tv_train[idx], origin='lower')\n",
    "# print(np.min(tv_val[idx]), np.max(tv_val[idx]))\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
