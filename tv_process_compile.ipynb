{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.io import readsav\n",
    "import h5py\n",
    "import pickle\n",
    "import cv2\n",
    "import diplib as dip\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brightness_reconstruction(img): # doi: 10.1109/TPS.2018.2828863.\n",
    "    im_norm = img / 255\n",
    "    img = np.average(im_norm,axis=None)\n",
    "    img = np.log(im_norm + 1) * (im_norm - img)\n",
    "    img = img / np.max(img)\n",
    "    img = np.where(img < 0, 0, img)\n",
    "    return img * 255\n",
    "\n",
    "def process_image(img, kernel_size, sigma, threshold, erode_kernel):\n",
    "    # img = cv2.GaussianBlur(img,(kernel_size, kernel_size),0)\n",
    "    img = brightness_reconstruction(img)\n",
    "    # img = np.array(dip.MatchedFiltersLineDetector2D(img, sigma = sigma)) # 10.1109/42.34715\n",
    "    img *= 255.0/img.max()\n",
    "    img = brightness_reconstruction(img)\n",
    "    img = np.where(img < threshold, 0, 1).astype('uint8')\n",
    "    # img = cv2.erode(img, np.ones((erode_kernel,erode_kernel), np.uint8), iterations=1)\n",
    "    return img\n",
    "\n",
    "def canny(img):\n",
    "    # gray=(255-255*(img-np.min(img))/(np.max(img)-np.min(img))).astype('uint8')\n",
    "\n",
    "    # reduce the noise using Gaussian filters\n",
    "    kernel_size = 11 \n",
    "    blur_gray = cv2.GaussianBlur(img,(kernel_size, kernel_size),0)\n",
    "\n",
    "    # Apply Canny edge detctor\n",
    "    low_threshold = 10\n",
    "    high_threshold = 20\n",
    "    edges = cv2.Canny(blur_gray, low_threshold, high_threshold)\n",
    "    \n",
    "    return edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "\n",
    "1. Loop across all files\n",
    "1. Loop across all indicies in file\n",
    "1. Get processed image, and r,l datapoints\n",
    "1. Append to 3 arrays\n",
    "1. After each full run, save process image array, r, l datapoint to hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_image_path = Path('tv_images')\n",
    "inversion_data_path = Path('outputs/inversion_data')\n",
    "hdf5_path = Path('outputs/hdf5')\n",
    "files = sorted(tv_image_path.glob('*.sav'))\n",
    "file_lengths = [len(readsav(str(file))['emission_structure'][0][3]) for file in files]\n",
    "cumulative_lengths = np.insert(np.cumsum(file_lengths), 0, 0)\n",
    "tv_dim = readsav(str(files[0]))['emission_structure'][0][7][0].shape\n",
    "inversion_dim = readsav(str(files[0]))['emission_structure'][0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tv_dim)\n",
    "print(inversion_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_dim = (240, 480)\n",
    "kernel_size = 5\n",
    "sigma = 1\n",
    "threshold = 4\n",
    "erode_kernel = 4\n",
    "aspect_num = 1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file_name = hdf5_path / 'tv_raw.hdf5'\n",
    "hf = h5py.File(hdf5_file_name, 'w') # open h5py file\n",
    "tv_dataset = hf.create_dataset(\"tv_images\", shape=(np.sum(file_lengths), tv_dim[0], tv_dim[1]), dtype='uint8')\n",
    "points_dataset = hf.create_dataset(\"points\", shape=(np.sum(file_lengths), 4), dtype='float32')\n",
    "\n",
    "# Add datasets to the groups\n",
    "for idx, file in enumerate(files):\n",
    "    frames = readsav(file)['emission_structure'][0][3].astype(int)\n",
    "    tv_image = readsav(file)['emission_structure'][0][7][frames]\n",
    "    tv_image_process = np.asarray(tv_image) # faster process and convert to binary\n",
    "    pkl_path = (inversion_data_path / file.stem).with_suffix('.pkl')\n",
    "    with open(pkl_path, 'rb') as pkl_file:\n",
    "            label_info = pickle.load(pkl_file)\n",
    "    points = np.concatenate((label_info['l_location'], label_info['r_location']),1)\n",
    "    \n",
    "    for i in range(file_lengths[idx]):\n",
    "        tv_dataset[cumulative_lengths[idx]+i] = tv_image_process[i]\n",
    "        points_dataset[cumulative_lengths[idx]+i] = points[i]\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file_name = hdf5_path / 'tv_process_simple.hdf5'\n",
    "hf = h5py.File(hdf5_file_name, 'w') # open h5py file\n",
    "tv_dataset = hf.create_dataset(\"tv_images\", shape=(np.sum(file_lengths), crop_dim[0], crop_dim[1]), dtype='uint8')\n",
    "points_dataset = hf.create_dataset(\"points\", shape=(np.sum(file_lengths), 4), dtype='float32')\n",
    "\n",
    "# Add datasets to the groups\n",
    "for idx, file in enumerate(files):\n",
    "    print(f\"{idx+1} of {len(files)}\")\n",
    "    frames = readsav(file)['emission_structure'][0][3].astype(int)\n",
    "    tv_image = readsav(file)['emission_structure'][0][7][frames]\n",
    "    pkl_path = (inversion_data_path / file.stem).with_suffix('.pkl')\n",
    "    with open(pkl_path, 'rb') as pkl_file:\n",
    "            label_info = pickle.load(pkl_file)\n",
    "    points = np.concatenate((label_info['l_location'], label_info['r_location']),1)\n",
    "    \n",
    "    for i in range(file_lengths[idx]):\n",
    "        tv_dataset[cumulative_lengths[idx]+i] = np.asarray(process_image(tv_image[i, 0:240, 240:720],kernel_size, sigma, threshold, erode_kernel))\n",
    "        points_dataset[cumulative_lengths[idx]+i] = points[i]\n",
    "    clear_output()\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file_name = hdf5_path / 'compiled_inversion_no_image.hdf5'\n",
    "hf = h5py.File(hdf5_file_name, 'w') # open h5py file\n",
    "rz_dataset = hf.create_dataset(\"rz\", shape=(np.sum(file_lengths), 4), dtype='float32')\n",
    "intensity_dataset = hf.create_dataset(\"intensity\", shape=(np.sum(file_lengths), 2), dtype='float32')\n",
    "\n",
    "# Add datasets to the groups\n",
    "for idx, file in enumerate(files):\n",
    "    pkl_path = (inversion_data_path / file.stem).with_suffix('.pkl')\n",
    "    with open(pkl_path, 'rb') as pkl_file:\n",
    "            label_info = pickle.load(pkl_file)\n",
    "    points = np.concatenate((label_info['l_location'], label_info['r_location']),1)\n",
    "    points_i = np.concatenate((label_info['l_intensity'], label_info['r_intensity']))\n",
    "\n",
    "    for i in range(file_lengths[idx]):\n",
    "            rz_dataset[cumulative_lengths[idx]+i] = points[i]\n",
    "            intensity_dataset[cumulative_lengths[idx]+i] = points_i[i]\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file_name = hdf5_path / 'x_outer_radiation.hdf5'\n",
    "hf = h5py.File(hdf5_file_name, 'w') # open h5py file\n",
    "tv_dataset = hf.create_dataset(\"tv_images\", shape=(np.sum(file_lengths), tv_dim[0], tv_dim[1]), dtype='uint8')\n",
    "points_dataset = hf.create_dataset(\"points\", shape=(np.sum(file_lengths), 4), dtype='float32')\n",
    "intensity_dataset = hf.create_dataset(\"intensity\", shape=(np.sum(file_lengths), 2), dtype='float32')\n",
    "\n",
    "# Add datasets to the groups\n",
    "for idx, file in enumerate(files):\n",
    "    frames = readsav(file)['emission_structure'][0][3].astype(int)\n",
    "    tv_image = readsav(file)['emission_structure'][0][7][frames]\n",
    "    tv_image_process = np.asarray(tv_image) # faster process and convert to binary\n",
    "    pkl_path = (inversion_data_path / file.stem).with_suffix('.pkl')\n",
    "    with open(pkl_path, 'rb') as pkl_file:\n",
    "            label_info = pickle.load(pkl_file)\n",
    "    points = np.concatenate((label_info['x_location'], label_info['r_location']),1)\n",
    "    points_i = np.concatenate((label_info['x_intensity'], label_info['r_intensity']))\n",
    "    \n",
    "    for i in range(file_lengths[idx]):\n",
    "        tv_dataset[cumulative_lengths[idx]+i] = tv_image_process[i]\n",
    "        points_dataset[cumulative_lengths[idx]+i] = points[i]\n",
    "        intensity_dataset[cumulative_lengths[idx]+i] = points_i[i]\n",
    "hf.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
