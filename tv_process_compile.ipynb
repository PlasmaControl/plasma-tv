{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.io import readsav\n",
    "import h5py\n",
    "import pickle\n",
    "import cv2\n",
    "# import diplib as dip\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import resize\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brightness_reconstruction(img): # doi: 10.1109/TPS.2018.2828863.\n",
    "    im_norm = img / 255\n",
    "    img = np.average(im_norm,axis=None)\n",
    "    img = np.log(im_norm + 1) * (im_norm - img)\n",
    "    img = img / np.max(img)\n",
    "    img = np.where(img < 0, 0, img)\n",
    "    return img * 255\n",
    "\n",
    "def process_image(img, kernel_size, sigma, threshold, erode_kernel):\n",
    "    # img = cv2.GaussianBlur(img,(kernel_size, kernel_size),0)\n",
    "    img = brightness_reconstruction(img)\n",
    "    # img = np.array(dip.MatchedFiltersLineDetector2D(img, sigma = sigma)) # 10.1109/42.34715\n",
    "    img *= 255.0/img.max()\n",
    "    img = brightness_reconstruction(img)\n",
    "    img = np.where(img < threshold, 0, 1).astype('uint8')\n",
    "    # img = cv2.erode(img, np.ones((erode_kernel,erode_kernel), np.uint8), iterations=1)\n",
    "    return img\n",
    "\n",
    "def canny(img):\n",
    "    # gray=(255-255*(img-np.min(img))/(np.max(img)-np.min(img))).astype('uint8')\n",
    "\n",
    "    # reduce the noise using Gaussian filters\n",
    "    kernel_size = 11 \n",
    "    blur_gray = cv2.GaussianBlur(img,(kernel_size, kernel_size),0)\n",
    "\n",
    "    # Apply Canny edge detctor\n",
    "    low_threshold = 10\n",
    "    high_threshold = 20\n",
    "    edges = cv2.Canny(blur_gray, low_threshold, high_threshold)\n",
    "    \n",
    "    return edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "\n",
    "1. Loop across all files\n",
    "1. Loop across all indicies in file\n",
    "1. Get processed image, and r,l datapoints\n",
    "1. Append to 3 arrays\n",
    "1. After each full run, save process image array, r, l datapoint to hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_image_path = Path('tv_images')\n",
    "inversion_data_path = Path('outputs/inversion_data')\n",
    "hdf5_path = Path('outputs/hdf5')\n",
    "files = sorted(tv_image_path.glob('*.sav'))\n",
    "file_lengths = [len(readsav(str(file))['emission_structure'][0][3]) for file in files]\n",
    "cumulative_lengths = np.insert(np.cumsum(file_lengths), 0, 0)\n",
    "tv_dim = readsav(str(files[0]))['emission_structure'][0][7][0].shape\n",
    "inversion_dim = readsav(str(files[0]))['emission_structure'][0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_dim = (240, 480)\n",
    "kernel_size = 5\n",
    "sigma = 1\n",
    "threshold = 4\n",
    "erode_kernel = 4\n",
    "aspect_num = 1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw and Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file_name = hdf5_path / 'tv_raw.hdf5'\n",
    "hf = h5py.File(hdf5_file_name, 'w') # open h5py file\n",
    "tv_dataset = hf.create_dataset(\"tv_images\", shape=(np.sum(file_lengths), tv_dim[0], tv_dim[1]), dtype='uint8')\n",
    "points_dataset = hf.create_dataset(\"points\", shape=(np.sum(file_lengths), 4), dtype='float32')\n",
    "\n",
    "# Add datasets to the groups\n",
    "for idx, file in enumerate(files):\n",
    "    frames = readsav(file)['emission_structure'][0][3].astype(int)\n",
    "    tv_image = readsav(file)['emission_structure'][0][7][frames]\n",
    "    tv_image_process = np.asarray(tv_image) # faster process and convert to binary\n",
    "    pkl_path = (inversion_data_path / file.stem).with_suffix('.pkl')\n",
    "    with open(pkl_path, 'rb') as pkl_file:\n",
    "            label_info = pickle.load(pkl_file)\n",
    "    points = np.concatenate((label_info['l_location'], label_info['r_location']),1)\n",
    "    \n",
    "    for i in range(file_lengths[idx]):\n",
    "        tv_dataset[cumulative_lengths[idx]+i] = tv_image_process[i]\n",
    "        points_dataset[cumulative_lengths[idx]+i] = points[i]\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file_name = hdf5_path / 'tv_process_simple.hdf5'\n",
    "hf = h5py.File(hdf5_file_name, 'w') # open h5py file\n",
    "tv_dataset = hf.create_dataset(\"tv_images\", shape=(np.sum(file_lengths), crop_dim[0], crop_dim[1]), dtype='uint8')\n",
    "points_dataset = hf.create_dataset(\"points\", shape=(np.sum(file_lengths), 4), dtype='float32')\n",
    "\n",
    "# Add datasets to the groups\n",
    "for idx, file in enumerate(files):\n",
    "    print(f\"{idx+1} of {len(files)}\")\n",
    "    frames = readsav(file)['emission_structure'][0][3].astype(int)\n",
    "    tv_image = readsav(file)['emission_structure'][0][7][frames]\n",
    "    pkl_path = (inversion_data_path / file.stem).with_suffix('.pkl')\n",
    "    with open(pkl_path, 'rb') as pkl_file:\n",
    "            label_info = pickle.load(pkl_file)\n",
    "    points = np.concatenate((label_info['l_location'], label_info['r_location']),1)\n",
    "    \n",
    "    for i in range(file_lengths[idx]):\n",
    "        tv_dataset[cumulative_lengths[idx]+i] = np.asarray(process_image(tv_image[i, 0:240, 240:720],kernel_size, sigma, threshold, erode_kernel))\n",
    "        points_dataset[cumulative_lengths[idx]+i] = points[i]\n",
    "    clear_output()\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inversion and Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file_name = hdf5_path / 'compiled_inversion_no_image.hdf5'\n",
    "hf = h5py.File(hdf5_file_name, 'w') # open h5py file\n",
    "rz_dataset = hf.create_dataset(\"rz\", shape=(np.sum(file_lengths), 4), dtype='float32')\n",
    "intensity_dataset = hf.create_dataset(\"intensity\", shape=(np.sum(file_lengths), 2), dtype='float32')\n",
    "\n",
    "# Add datasets to the groups\n",
    "for idx, file in enumerate(files):\n",
    "    pkl_path = (inversion_data_path / file.stem).with_suffix('.pkl')\n",
    "    with open(pkl_path, 'rb') as pkl_file:\n",
    "            label_info = pickle.load(pkl_file)\n",
    "    points = np.concatenate((label_info['l_location'], label_info['r_location']),1)\n",
    "    points_i = np.concatenate((label_info['l_intensity'], label_info['r_intensity']))\n",
    "\n",
    "    for i in range(file_lengths[idx]):\n",
    "            rz_dataset[cumulative_lengths[idx]+i] = points[i]\n",
    "            intensity_dataset[cumulative_lengths[idx]+i] = points_i[i]\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw and Synthetic and Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file_name = hdf5_path / 'x_outer_radiation.hdf5'\n",
    "hf = h5py.File(hdf5_file_name, 'w') # open h5py file\n",
    "tv_dataset = hf.create_dataset(\"tv_images\", shape=(np.sum(file_lengths), tv_dim[0], tv_dim[1]), dtype='uint8')\n",
    "points_dataset = hf.create_dataset(\"points\", shape=(np.sum(file_lengths), 4), dtype='float32')\n",
    "intensity_dataset = hf.create_dataset(\"intensity\", shape=(np.sum(file_lengths), 2), dtype='float32')\n",
    "print(files)\n",
    "# Add datasets to the groups\n",
    "for idx, file in enumerate(files):\n",
    "    frames = readsav(file)['emission_structure'][0][3].astype(int)\n",
    "    tv_image = readsav(file)['emission_structure'][0][7][frames]\n",
    "    tv_image_process = np.asarray(tv_image) # faster process and convert to binary\n",
    "    pkl_path = (inversion_data_path / file.stem).with_suffix('.pkl')\n",
    "    with open(pkl_path, 'rb') as pkl_file:\n",
    "            label_info = pickle.load(pkl_file)\n",
    "    points = np.concatenate((label_info['x_location'], label_info['r_location']),1)\n",
    "    points_i = np.concatenate((label_info['x_intensity'], label_info['r_intensity']))\n",
    "    \n",
    "    for i in range(file_lengths[idx]):\n",
    "        tv_dataset[cumulative_lengths[idx]+i] = tv_image_process[i]\n",
    "        points_dataset[cumulative_lengths[idx]+i] = points[i]\n",
    "        intensity_dataset[cumulative_lengths[idx]+i] = points_i[i]\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw and Inversion and Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anti_file_lengths = [len(readsav(str(file))['emission_structure'][0][5]) - len(readsav(str(file))['emission_structure'][0][3]) for file in files]\n",
    "anti_cum_len = np.insert(np.cumsum(anti_file_lengths), 0, 0)\n",
    "tv_full_train = np.zeros((np.sum(file_lengths), 256, 256))\n",
    "inversion_full_train = np.zeros((np.sum(file_lengths), 256, 256))\n",
    "tv_full_test = np.zeros((np.sum(anti_file_lengths), 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, file in enumerate(files):\n",
    "    frames = readsav(file)['emission_structure'][0][3].astype(int)\n",
    "    tv_image = readsav(file)['emission_structure'][0][7][frames]\n",
    "    inversion = readsav(file)['emission_structure'][0][0]\n",
    "    file_len = len(frames)\n",
    "    resize_tv = np.flip(resize(tv_image, (file_len, 256, 256), order=0, preserve_range=True), axis=(2,1))\n",
    "    resize_inversion = resize(inversion, (file_len, 256, 256), order=0, preserve_range=True)\n",
    "    tv_full_train[cumulative_lengths[idx]:cumulative_lengths[idx+1]] = resize_tv / 127.5 - 1\n",
    "    inversion_full_train[cumulative_lengths[idx]:cumulative_lengths[idx+1]] = resize_inversion / 127.5 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, file in enumerate(files):\n",
    "    frames = np.setdiff1d(readsav(file)['emission_structure'][0][5].astype(int), readsav(file)['emission_structure'][0][3].astype(int))\n",
    "    tv_image = readsav(file)['emission_structure'][0][7][frames]\n",
    "    file_len = len(frames)\n",
    "    resize_tv = np.flip(resize(tv_image, (file_len, 256, 256), order=0, preserve_range=True), axis=(2,1))\n",
    "    tv_full_test[anti_cum_len[idx]:anti_cum_len[idx+1]] = resize_tv / 127.5 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(tv_full_train[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tv_full_train.shape)\n",
    "print(inversion_full_train.shape)\n",
    "print(tv_full_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = Path('outputs')\n",
    "with h5py.File(out_path / 'tv_inversion.h5', 'w') as f:\n",
    "    f.create_dataset('A_train', data=tv_full_train)\n",
    "    f.create_dataset('A_test', data=tv_full_test)\n",
    "    f.create_dataset('B_train', data=inversion_full_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TV Synthetic HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomization = False\n",
    "\n",
    "file_name_1 = Path('outputs/hdf5/s_outs_v3_limited.h5')\n",
    "file_name_2 = Path('outputs/hdf5/x_outer_radiation.hdf5')\n",
    "\n",
    "out_path = Path('outputs')\n",
    "\n",
    "with h5py.File(file_name_1, 'r') as f:\n",
    "    synthetic_images = f['image'][:] * 2 - 1\n",
    "    \n",
    "with h5py.File(file_name_2, 'r') as f:\n",
    "    points = f['points'][:]\n",
    "    tv_images = f['tv_images'][:] / 127.5 - 1\n",
    "\n",
    "print(len(synthetic_images), len(tv_images))\n",
    "file_len = 1840\n",
    "\n",
    "crop_synthetic = resize(synthetic_images[:file_len], (file_len, 256, 256))\n",
    "crop_tv = np.flip(resize(tv_images[:file_len], (file_len, 256, 256)), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(synthetic_images.min(), synthetic_images.max())\n",
    "print(tv_images.min(), tv_images.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1799\n",
    "axs1 = plt.subplot(1,2,1)\n",
    "axs1.imshow(crop_synthetic[idx], origin='lower')\n",
    "print(np.min(crop_synthetic[idx]), np.max(crop_synthetic[idx]))\n",
    "\n",
    "axs2 = plt.subplot(1,2,2)\n",
    "axs2.imshow(crop_tv[idx], origin='lower')\n",
    "print(np.min(crop_tv[idx]), np.max(crop_tv[idx]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(synth_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tts_percent = 0.99\n",
    "tvs_percent = 0.8\n",
    "\n",
    "# synth_dat, synth_val, tv_dat, tv_val = train_test_split(crop_synthetic, crop_tv, train_size=tts_percent, random_state=42)\n",
    "synth_train, synth_test, tv_train, tv_test = train_test_split(crop_synthetic, crop_tv, train_size=tvs_percent, random_state=42)\n",
    "\n",
    "# print(len(synth_dat), len(synth_val), len(tv_dat), len(tv_val))\n",
    "print(len(synth_train), len(synth_test), len(tv_train), len(tv_test))\n",
    "with h5py.File(out_path / 'tv_synth.h5', 'w') as f:\n",
    "    f.create_dataset('synth_train', data=synth_train)\n",
    "    f.create_dataset('synth_test', data=synth_test)\n",
    "    # f.create_dataset('synth_val', data=synth_val)\n",
    "    f.create_dataset('tv_train', data=tv_train)\n",
    "    f.create_dataset('tv_test', data=tv_test)\n",
    "    # f.create_dataset('tv_val', data=tv_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axs1 = plt.subplot(1,2,1)\n",
    "axs1.imshow(crop_tv[211], origin='lower')\n",
    "axs2 = plt.subplot(1,2,2)\n",
    "axs2.imshow(crop_synthetic[10], origin='lower')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'outputs/tv_synth.h5'\n",
    "\n",
    "with h5py.File(file_name, 'r') as f:\n",
    "    synth_train = f['synth_train'][:]\n",
    "    synth_test = f['synth_test'][:]\n",
    "    # synth_val = f['synth_val'][:]\n",
    "    tv_train = f['tv_train'][:]\n",
    "    tv_test = f['tv_test'][:]\n",
    "    # tv_val = f['tv_val'][:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 884\n",
    "axs1 = plt.subplot(1,2,1)\n",
    "axs1.imshow(synth_train[idx], origin='lower')\n",
    "# print(np.min(synth_val[idx]), np.max(synth_val[idx]))\n",
    "\n",
    "axs2 = plt.subplot(1,2,2)\n",
    "axs2.imshow(tv_train[idx], origin='lower')\n",
    "# print(np.min(tv_val[idx]), np.max(tv_val[idx]))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
